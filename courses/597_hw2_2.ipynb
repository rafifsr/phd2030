{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e9b0c39",
   "metadata": {},
   "source": [
    "# HW2 Problem 2: BFGS\n",
    "\n",
    "We want to optimize the function $f: \\mathbb{R}^3 \\rightarrow \\mathbb{R}$\n",
    "\n",
    "$$f(x_1,x_2,x_3) = x_3\\log(e^{\\frac{x_1}{x_3}}+e^{\\frac{x_2}{x_3}})+(x_3 - 2)^2+e^{\\frac{1}{x_1+x_2}}$$\n",
    "$$\\textbf{dom } f=\\{\\mathbf{x}\\in\\mathbb{R}^3 : x_1+x_2>0,\\,x_3>0\\}$$\n",
    "\n",
    "using the BFGS algorithm.\n",
    "\n",
    "- Given starting point $x_0 = [2,3,5]^T$, convergence tolerance $\\epsilon > 0$, starting matrix $H_0 = \\mathbf{I}$, $k\\leftarrow 0$\n",
    "- While $||\\nabla f_k||>\\epsilon$:\n",
    "    1. Get direction by solving $$p_k = -H_k^{-1} \\nabla f_k$$\n",
    "    2. Update $x_{k+1} = x_k + \\alpha_k p_k$ where $\\alpha_k$ is obtained from backtracking\n",
    "    3. Define $s_k = x_{k+1} - x_k$ and $y_k = \\nabla f_{k+1} - \\nabla f_k$\n",
    "    4. Compute $$H_{k+1} = (I - \\rho_k s_k y_k^T)H_k(I - \\rho_k s_k y_k^T) + \\rho_k s_k s_k^T$$\n",
    "- We can take $\\epsilon = 10^{-4}$ and a constant $\\rho_k = 10^4$\n",
    "\n",
    "$$\\nabla f(x_1,x_2,x_3)=\n",
    "\\begin{pmatrix}\n",
    "\\sigma \\left(\\dfrac{x_1 - x_2}{x_3}\\right)\n",
    "-\\dfrac{e^{\\frac{1}{x_1+x_2}}}{(x_1+x_2)^2}\n",
    "\\\\[1.2em]\n",
    "\\sigma \\left(-\\dfrac{x_1 - x_2}{x_3}\\right)\n",
    "-\\dfrac{e^{\\frac{1}{x_1+x_2}}}{(x_1+x_2)^2}\n",
    "\\\\[1.2em]\n",
    "\\log\\!\\big(e^{x_1/x_3}+e^{x_2/x_3}\\big)\n",
    "-\\dfrac{x_2 + (x_1 - x_2)\\sigma \\left(\\dfrac{x_1 - x_2}{x_3}\\right)}\n",
    "{x_3}\n",
    "+2(x_3-2)\n",
    "\\end{pmatrix},$$\n",
    "\n",
    "with the Sigmoid function $$\\sigma(z) = \\frac{1}{1+\\exp(-z)}.$$\n",
    "\n",
    "The finite difference approximation for the gradient at a point $x$ is: $$\\nabla f(x) \\approx \\frac{f(x+h\\mathbf{i})-f(x)}{h}$$\n",
    "\n",
    "And the second derivative matrix is approximated as: $$\\nabla^2 f(x) \\approx \\frac{\\nabla f(x+h\\mathbf{i}) - \\nabla f(x)}{h}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "92a7c4be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import numpy as np\n",
    "from numpy import log, exp\n",
    "from scipy.special import expit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb8b4bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function\n",
    "def f(x:np.ndarray) -> float:\n",
    "    x1, x2, x3 = x \n",
    "    val = x3 * np.log(np.exp(x1 / x3) + np.exp(x2 / x3)) + (x3 - 2) ** 2 + np.exp(1 / (x1 + x2))\n",
    "    return val\n",
    "\n",
    "# Define the gradient of the function\n",
    "def grad_f(x:np.ndarray) -> np.ndarray:\n",
    "    \n",
    "    x1, x2, x3 = x\n",
    "    delta = (x1 - x2) / x3\n",
    "    E = exp(1 / (x1 + x2))\n",
    "    q = (x1 + x2)**2\n",
    "\n",
    "    grad = np.zeros(3)\n",
    "    grad[0] = expit(delta) - E / q\n",
    "    grad[1] = expit(-delta) - E / q\n",
    "    grad[2] = log(exp(x1 / x3) + exp(x2 / x3)) - (2 - x3) * 2 - (x2 + (x1 - x2)*expit(delta)) / x3\n",
    "\n",
    "    return grad\n",
    "\n",
    "# Define the approximate Hessian of the function\n",
    "def hess_f(x:np.ndarray) -> np.ndarray:\n",
    "    x = x.flatten()\n",
    "    h = 1e-2  # Step size\n",
    "    id = np.eye(len(x))  # Identity matrix\n",
    "\n",
    "    # Construct the perturbation matrix with h values along the diagonal\n",
    "    h_matrix = h * id\n",
    "\n",
    "    # Calculate the forward differences for all components simultaneously\n",
    "    perturbed_values = np.array([grad_f(x + h_vec) for h_vec in h_matrix])\n",
    "\n",
    "    # Calculate the second derivative approximation\n",
    "    approx_hessian = (perturbed_values - grad_f(x)) / h\n",
    "    \n",
    "    reshaped_approx_hessian = np.reshape(approx_hessian, (3, 3))\n",
    "    \n",
    "    return reshaped_approx_hessian "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5efc851f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal solution: x = [0.92618691 0.92623001 1.65342475], f(x) = 3.908113786405081\n",
      "Number of iterations: 142\n"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "alp = 0.4\n",
    "beta = 0.5\n",
    "eps = 10**(-5)\n",
    "x_start = np.array([2,3,5])\n",
    "\n",
    "# Domain check function\n",
    "def in_domain(x: np.ndarray) -> bool:\n",
    "    return (x[2] > 0) and ((x[0] + x[1]) > 0)\n",
    "\n",
    "# BFGS Direction function\n",
    "def bfgs_direction(x: np.ndarray, H: np.ndarray) -> np.ndarray:\n",
    "    grad = grad_f(x)\n",
    "    p = -np.linalg.solve(H, grad)\n",
    "    return p\n",
    "\n",
    "# Backtracking Line Search function\n",
    "def backtracking_line_search(x: np.ndarray, p: np.ndarray, alpha: float, beta: float) -> float:\n",
    "    t = 1.0\n",
    "    while not in_domain(x + t * p) or f(x + t * p) > f(x) + alpha * t * np.dot(grad_f(x), p):\n",
    "        t *= beta\n",
    "    return t\n",
    "\n",
    "# BFGS Algorithm\n",
    "def bfgs_algorithm(x_start: np.ndarray, alp: float, beta: float, eps: float) -> np.ndarray:\n",
    "    x = x_start\n",
    "    H = np.eye(len(x_start))  # Initial Hessian approximation\n",
    "    iteration = 0\n",
    "\n",
    "    while np.linalg.norm(grad_f(x)) > eps:\n",
    "        p = bfgs_direction(x, H)\n",
    "        t = backtracking_line_search(x, p, alp, beta)\n",
    "        s = t * p\n",
    "        x_new = x + s\n",
    "        y = grad_f(x_new) - grad_f(x)\n",
    "\n",
    "        if np.dot(y, s) > 1e-10:  # To ensure numerical stability\n",
    "            rho = 1.0 / np.dot(y, s)\n",
    "            H = (np.eye(len(x_start)) - rho * np.outer(s, y)) @ H @ (np.eye(len(x_start)) - rho * np.outer(y, s)) + rho * np.outer(s, s)\n",
    "\n",
    "        x = x_new\n",
    "        iteration += 1\n",
    "\n",
    "    return x, iteration\n",
    "\n",
    "# Run the BFGS algorithm\n",
    "optimal_x, iteration = bfgs_algorithm(x_start, alp, beta, eps)\n",
    "print(f\"Optimal solution: x = {optimal_x}, f(x) = {f(optimal_x)}\")\n",
    "print(f\"Number of iterations: {iteration}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "phd2030 (3.12.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
