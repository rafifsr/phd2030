{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "781ae9be",
   "metadata": {},
   "source": [
    "# HW2 Problem 1: \n",
    "\n",
    "We want to optimize the function $f: \\mathbb{R}^3 \\rightarrow \\mathbb{R}$\n",
    "\n",
    "$$f(x_1,x_2,x_3) = x_3\\log(e^{\\frac{x_1}{x_3}}+e^{\\frac{x_2}{x_3}})+(x_3 - 2)^2+e^{\\frac{1}{x_1+x_2}}$$\n",
    "$$\\textbf{dom } f=\\{\\mathbf{x}\\in\\mathbb{R}^3 : x_1+x_2>0,\\,x_3>0\\}$$\n",
    "\n",
    "The gradient is:\n",
    "\n",
    "$$\\nabla f(x_1,x_2,x_3)=\n",
    "\\begin{pmatrix}\n",
    "\\dfrac{e^{x_1/x_3}}{e^{x_1/x_3}+e^{x_2/x_3}}\n",
    "-\\dfrac{e^{\\frac{1}{x_1+x_2}}}{(x_1+x_2)^2}\n",
    "\\\\[1.2em]\n",
    "\\dfrac{e^{x_2/x_3}}{e^{x_1/x_3}+e^{x_2/x_3}}\n",
    "-\\dfrac{e^{\\frac{1}{x_1+x_2}}}{(x_1+x_2)^2}\n",
    "\\\\[1.2em]\n",
    "\\log\\!\\big(e^{x_1/x_3}+e^{x_2/x_3}\\big)\n",
    "-\\dfrac{x_1 e^{x_1/x_3}+x_2 e^{x_2/x_3}}\n",
    "{x_3\\big(e^{x_1/x_3}+e^{x_2/x_3}\\big)}\n",
    "+2(x_3-2)\n",
    "\\end{pmatrix}$$\n",
    "\n",
    "To simplify the gradient, we can use the Sigmoid function `scipy.special.expit`\n",
    "$$\\sigma(z) = \\frac{1}{1+\\exp(-z)}$$\n",
    "\n",
    "Now the gradient is:\n",
    "\n",
    "$$\\nabla f(x_1,x_2,x_3)=\n",
    "\\begin{pmatrix}\n",
    "\\sigma \\left(\\dfrac{x_1 - x_2}{x_3}\\right)\n",
    "-\\dfrac{e^{\\frac{1}{x_1+x_2}}}{(x_1+x_2)^2}\n",
    "\\\\[1.2em]\n",
    "\\sigma \\left(-\\dfrac{x_1 - x_2}{x_3}\\right)\n",
    "-\\dfrac{e^{\\frac{1}{x_1+x_2}}}{(x_1+x_2)^2}\n",
    "\\\\[1.2em]\n",
    "\\log\\!\\big(e^{x_1/x_3}+e^{x_2/x_3}\\big)\n",
    "-\\dfrac{x_2 + (x_1 - x_2)\\sigma \\left(\\dfrac{x_1 - x_2}{x_3}\\right)}\n",
    "{x_3}\n",
    "+2(x_3-2)\n",
    "\\end{pmatrix}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5bf81f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import numpy as np\n",
    "from numpy import log, exp\n",
    "from scipy.special import expit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a63b008",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function\n",
    "def f(x:np.ndarray) -> float:\n",
    "    x1, x2, x3 = x \n",
    "    val = x3 * np.log(np.exp(x1 / x3) + np.exp(x2 / x3)) + (x3 - 2) ** 2 + np.exp(1 / (x1 + x2))\n",
    "    return val\n",
    "\n",
    "# Define the gradient of the function\n",
    "def grad_f(x:np.ndarray) -> np.ndarray:\n",
    "    \n",
    "    x1, x2, x3 = x\n",
    "    delta = (x1 - x2) / x3\n",
    "    E = exp(1 / (x1 + x2))\n",
    "    q = (x1 + x2)**2\n",
    "\n",
    "    grad = np.zeros(3)\n",
    "    grad[0] = expit(delta) - E / q\n",
    "    grad[1] = expit(-delta) - E / q\n",
    "    grad[2] = log(exp(x1 / x3) + exp(x2 / x3)) - (2 - x3) * 2 - (x2 + (x1 - x2)*expit(delta)) / x3\n",
    "\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffad3786",
   "metadata": {},
   "source": [
    "## Gradient Descent with Backtracking Line Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "90862d8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal solution: [0.92618727 0.92622965 1.65342641]\n",
      "Optimal function value: 3.9081137863976365\n",
      "Number of iterations taken to converge: 30\n"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "alpha = 0.4\n",
    "beta = 0.5\n",
    "eps = 1e-5\n",
    "x0 = np.array([3, 4, 5])\n",
    "\n",
    "# Ensuring we stay within the domain\n",
    "def domain(x:np.ndarray) -> float:\n",
    "    t = 1\n",
    "    while True:\n",
    "        v = x - t * grad_f(x).flatten()\n",
    "        e3 = v[2]\n",
    "        e2 = v[1]\n",
    "        e1 = v[0]\n",
    "\n",
    "        if e3 > 0 and (e1 + e2 > 0):\n",
    "            return t # Exit the loop and return t if in domain\n",
    "        \n",
    "        t *= beta # Else, reduce t and try again\n",
    "    \n",
    "# Backtracking line search\n",
    "def backtracking(x:np.ndarray) -> float:\n",
    "    t = domain(x)\n",
    "    grad_fx = grad_f(x).flatten()\n",
    "    xv = x - t * grad_fx\n",
    "    lhs = f(xv)\n",
    "    rhs = f(x) - alpha * t * np.dot(grad_fx, grad_fx)\n",
    "\n",
    "    while lhs > rhs:\n",
    "        t *= beta\n",
    "        xv = x - t * grad_fx\n",
    "        lhs = f(xv)\n",
    "        rhs = f(x) - alpha * t * np.dot(grad_fx, grad_fx)\n",
    "\n",
    "    return t\n",
    "\n",
    "# Gradient descent with backtracking line search\n",
    "grad_fx = grad_f(x0).flatten()\n",
    "norm = np.dot(grad_fx, grad_fx)**0.5\n",
    "iter = 0\n",
    "while norm > eps:\n",
    "    direction = -grad_fx\n",
    "    t = backtracking(x0)\n",
    "    x0 = x0 + t * direction\n",
    "    grad_fx = grad_f(x0).flatten()\n",
    "    norm = np.dot(grad_fx, grad_fx)**0.5\n",
    "    iter += 1\n",
    "\n",
    "print(\"Optimal solution:\", x0)\n",
    "print(\"Optimal function value:\", f(x0))\n",
    "print(\"Number of iterations taken to converge:\", iter)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "phd2030 (3.12.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
